# -*- coding: utf-8 -*-
"""AML_MINI_project-text_dataset

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CHFhZwzcMRlDD3dMuarmf0gAf-Svqr9T
"""

import pandas as pd
df=pd.read_csv("/content/survey lung cancer.csv")
df

#Selected Features:
#Index(['AGE', 'YELLOW_FINGERS', 'PEER_PRESSURE', 'CHRONIC DISEASE', 'FATIGUE ','ALLERGY ', 'WHEEZING', 'ALCOHOL CONSUMING', 'COUGHING','SWALLOWING DIFFICULTY'],

df.head()

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
df['GENDER']= label_encoder.fit_transform(df['GENDER'])
df['LUNG_CANCER']= label_encoder.fit_transform(df['LUNG_CANCER'])

df.head()

df.describe().transpose()

x=df.iloc[:,1:11]
y=df['LUNG_CANCER']
y

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=56)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x2_train = scaler.fit_transform(x_train)
x2_test= scaler.transform(x_test)

from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(x_train,y_train)

y_pred=dt.predict(x_test)
y_pred

from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,accuracy_score

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred))
print("R2_score :",r2_score(y_test,y_pred))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred))

from sklearn.metrics import confusion_matrix,classification_report
cm = confusion_matrix(y_test,y_pred)
cr= classification_report(y_test,y_pred)
a=accuracy_score(y_test, y_pred)
print(cm)
print(cr)
print(a)

import matplotlib.pyplot as plt
from sklearn import metrics
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None,min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None,bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False,class_weight=None)
rf.fit(x_train,y_train)

y_pred2=rf.predict(x_test)
y_pred2

from sklearn.feature_selection import SelectFromModel
feature_importances = rf.feature_importances_
selector = SelectFromModel(rf, threshold=0.05)  # You can adjust the threshold as needed

# Apply feature selection
selector.fit(x_train, y_train)
selected_features = x.columns[selector.get_support()]

# Print the selected features
print("Selected Features:")
print(selected_features)

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred2))
print("R2_score :",r2_score(y_test,y_pred2))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred2))

cm2 = confusion_matrix(y_test,y_pred2)
cr2= classification_report(y_test,y_pred2)
print(cm2)
print(cr2)
a2=accuracy_score(y_test, y_pred2)
print(a2)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm2, display_labels = [False, True])
cm_display.plot()
plt.show()

from sklearn.naive_bayes import GaussianNB
gb = GaussianNB()
gb.fit(x_train, y_train)

y_pred3=gb.predict(x_test)
y_pred3

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred3))
print("R2_score :",r2_score(y_test,y_pred3))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred3))

from mpl_toolkits.mplot3d.axes3d import Axes3D
cm3 = confusion_matrix(y_test,y_pred3)
cr3= classification_report(y_test,y_pred3)
print(cm3)
print(cr3)
a3=accuracy_score(y_test, y_pred3)
print(a3)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm3, display_labels = [False, True])
cm_display.plot()
plt.show()

# Building a Support Vector Machine on train data
from sklearn.svm import SVC
svc = SVC(C=.1, kernel='linear', gamma=1)
svc.fit(x_train, y_train)

y_pred4=svc.predict(x_test)
y_pred4

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred4))
print("R2_score :",r2_score(y_test,y_pred4))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred4))

cm4 = confusion_matrix(y_test,y_pred4)
cr4= classification_report(y_test,y_pred4)
print(cm4)
print(cr4)
a4=accuracy_score(y_test, y_pred4)
print(a4)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm4, display_labels = [False, True])
cm_display.plot()
plt.show()

from sklearn.neighbors import KNeighborsClassifier
knn= KNeighborsClassifier(n_neighbors=5)
knn.fit(x_train, y_train)

y_pred5=knn.predict(x_test)
y_pred5

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred5))
print("R2_score :",r2_score(y_test,y_pred5))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred5))

cm5 = confusion_matrix(y_test,y_pred5)
cr5= classification_report(y_test,y_pred5)
print(cm5)
print(cr5)
a5=accuracy_score(y_test, y_pred5)
print(a5)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm5, display_labels = [False, True])
cm_display.plot()
plt.show()

from sklearn.linear_model import LogisticRegression
 lr=LogisticRegression(random_state=5)
 lr.fit(x_train, y_train)

y_pred6=lr.predict(x_test)
y_pred6

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred6))
print("R2_score :",r2_score(y_test,y_pred6))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred6))

cm6 = confusion_matrix(y_test,y_pred6)
cr6= classification_report(y_test,y_pred6)
print(cm6)
print(cr6)
a6=accuracy_score(y_test, y_pred6)
print(a6)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm6, display_labels = [False, True])
cm_display.plot()
plt.show()

from sklearn.ensemble import AdaBoostClassifier
 ad=AdaBoostClassifier(n_estimators=10, random_state=7)

ad.fit(x_train,y_train)
y_pred7=ad.predict(x_test)
y_pred7

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred7))
print("R2_score :",r2_score(y_test,y_pred7))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred7))

cm7= confusion_matrix(y_test,y_pred7)
cr7= classification_report(y_test,y_pred7)
print(cm7)
print(cr7)
a7=accuracy_score(y_test, y_pred7)
print(a7)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm7, display_labels = [False, True])
cm_display.plot()
plt.show()

import xgboost as xgb
xg= xgb.XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.05,
                max_depth = 10, alpha = 1, n_estimators = 100)
xg.fit(x_train, y_train)

y_pred8=xg.predict(x_test)
y_pred8

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred8))
print("R2_score :",r2_score(y_test,y_pred8))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred8))

cm8= confusion_matrix(y_test,y_pred8)
cr8= classification_report(y_test,y_pred8)
print(cm8)
print(cr8)
a8=accuracy_score(y_test, y_pred8)
print(a8)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm8, display_labels = [False, True])
cm_display.plot()
plt.show()

from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
estimator = []
estimator.append(('SVC', SVC(gamma ='auto', probability = True)))
estimator.append(('RF', RandomForestClassifier()))
from sklearn.ensemble import VotingClassifier
vot_hard = VotingClassifier( estimators = estimator,voting ='hard')
vot_hard.fit(x_train, y_train)

y_pred9 = vot_hard.predict(x_test)
y_pred9

print("Mean_Squared_Error :",mean_squared_error(y_test,y_pred9))
print("R2_score :",r2_score(y_test,y_pred9))
print("Mean_Absolute_Error :",mean_absolute_error(y_test,y_pred9))

cm9= confusion_matrix(y_test,y_pred9)
cr9= classification_report(y_test,y_pred9)
print(cm9)
print(cr9)
a9=accuracy_score(y_test, y_pred9)
print(a9)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm9, display_labels = [False, True])
cm_display.plot()
plt.show()

ml=["Decision Tree","Random Forest","Naive Bayes","SVC","KNN","Logistic Regression","AdaBoost","XGBoost","VotingClassifier"]
ac=[a,a2,a3,a4,a5,a6,a7,a8,a9]

ml

fig = plt.figure(figsize = (10, 5))

# creating the bar plot
plt.bar(ml,ac,
		width = 0.4,color=['g','r','b','cyan','magenta','yellow','silver','black','purple'])

plt.xlabel("ML Models")
plt.ylabel("Accuracy")
plt.title("Accuracy score of ML models")
plt.show()

